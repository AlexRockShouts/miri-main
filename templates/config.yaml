storage_dir: ~/.miri

server:
  addr: :8080
  key: local-dev-key
  host: localhost
  admin_user: admin
  admin_pass: admin-password

miri:
  brain:
    embeddings:
      # is use_native_embeddings true, a local -static - algorithm is used based on a qwen3_embedding06b model
      use_native_embeddings: true
      # use_native_embeddings: false use one of the external embeddings below
      model:
        # Included types: "openai", "openai-compatible, ""mistral", "cohere", "ollama", "jina", "mixedbread", "localai", "azure-openai"
        # if openai-compatible, use the openai-compatible model name, e.g. "gpt-3.5-turbo" and provide an URL
        type: none
        api_key: "$NONE_API_KEY"


models:
  mode: merge
  providers:
    xai:
      baseUrl: https://api.x.ai/v1
      apiKey: "$XAI_API_KEY"
      api: openai
      models:
        - id: xai/grok-4-1-fast-reasoning
          name: grok-4-1-fast-reasoning
          reasoning: true
          input: [text]
          cost: {input: 0.20, output: 0.50, cacheRead: 0, cacheWrite: 0}
          contextWindow: 2000000
          maxTokens: 8192
        - id: xai/grok-4-1-fast-non-reasoning
          name: grok-4-1-fast-non-reasoning
          reasoning: false
          input: [text]
          cost: {input: 0.20, output: 0.50, cacheRead: 0, cacheWrite: 0}
          contextWindow: 2000000
          maxTokens: 8192
        - id: xai/grok-code-fast-1
          name: grok-code-fast-1
          reasoning: false
          input: [text]
          cost: {input: 0.20, output: 1.50, cacheRead: 0, cacheWrite: 0}
          contextWindow: 256000
          maxTokens: 8192
        - id: xai/grok-4-fast-reasoning
          name: grok-4-fast-reasoning
          reasoning: true
          input: [text]
          cost: {input: 0.20, output: 0.50, cacheRead: 0, cacheWrite: 0}
          contextWindow: 2000000
          maxTokens: 8192
        - id: xai/grok-4-fast-non-reasoning
          name: grok-4-fast-non-reasoning
          reasoning: false
          input: [text]
          cost: {input: 0.20, output: 0.50, cacheRead: 0, cacheWrite: 0}
          contextWindow: 2000000
          maxTokens: 8192
        - id: xai/grok-4-0709
          name: grok-4-0709
          reasoning: false
          input: [text]
          cost: {input: 3.00, output: 15.00, cacheRead: 0, cacheWrite: 0}
          contextWindow: 256000
          maxTokens: 8192
        - id: xai/grok-3-mini
          name: grok-3-mini
          reasoning: false
          input: [text]
          cost: {input: 0.30, output: 0.50, cacheRead: 0, cacheWrite: 0}
          contextWindow: 131072
          maxTokens: 8192
        - id: xai/grok-3
          name: grok-3
          reasoning: false
          input: [text]
          cost: {input: 3.00, output: 15.00, cacheRead: 0, cacheWrite: 0}
          contextWindow: 131072
          maxTokens: 8192
        - id: xai/grok-2-vision-1212
          name: grok-2-vision-1212
          reasoning: false
          input: [text, image]
          cost: {input: 2.00, output: 10.00, cacheRead: 0, cacheWrite: 0}
          contextWindow: 32768
          maxTokens: 8192
        - id: xai/grok-imagine-image-pro
          name: grok-imagine-image-pro
          reasoning: false
          input: [text]
          cost: {input: 0.03, output: 0.03, cacheRead: 0, cacheWrite: 0}
          contextWindow: 4096
          maxTokens: 512
        - id: xai/grok-imagine-image
          name: grok-imagine-image
          reasoning: false
          input: [text]
          cost: {input: 0.03, output: 0.03, cacheRead: 0, cacheWrite: 0}
          contextWindow: 4096
          maxTokens: 512
        - id: xai/grok-2-image-1212
          name: grok-2-image-1212
          reasoning: false
          input: [text]
          cost: {input: 0.07, output: 0.07, cacheRead: 0, cacheWrite: 0}
          contextWindow: 4096
          maxTokens: 512
        - id: xai/grok-imagine-video
          name: grok-imagine-video
          reasoning: false
          input: [text]
          cost: {input: 0.05, output: 0.05, cacheRead: 0, cacheWrite: 0}
          contextWindow: 4096
          maxTokens: 512
    huggingface:
      baseUrl: https://api-inference.huggingface.co/v1/
      apiKey: "$HF_TOKEN"
      api: openai
      models:
        - id: meta-llama/Llama-3.3-70B-Instruct
          name: Llama 3.3 70B Instruct
          reasoning: false
          input: [text]
          cost: {input: 0, output: 0, cacheRead: 0, cacheWrite: 0}
          contextWindow: 128000
          maxTokens: 4096
        - id: mistralai/Mistral-7B-Instruct-v0.3
          name: Mistral 7B Instruct v0.3
          reasoning: false
          input: [text]
          cost: {input: 0, output: 0, cacheRead: 0, cacheWrite: 0}
          contextWindow: 32768
          maxTokens: 4096
        - id: Qwen/Qwen2.5-72B-Instruct
          name: Qwen 2.5 72B Instruct
          reasoning: false
          input: [text]
          cost: {input: 0, output: 0, cacheRead: 0, cacheWrite: 0}
          contextWindow: 128000
          maxTokens: 4096
    nvidia:
      baseUrl: https://integrate.api.nvidia.com/v1
      apiKey: "$NVIDIA_API_KEY"
      api: openai-completions
      models:
        - id: kimi-k2.5
          name: Kimi K2.5
          reasoning: false
          input:
            - text
          cost:
            input: 0
            output: 0
            cacheRead: 0
            cacheWrite: 0
          contextWindow: 131072
          maxTokens: 8192

agents:
  debug: true
  defaults:
    model:
      primary: xai/grok-4-1-fast-reasoning
      fallbacks: [xai/grok-4-1-fast-non-reasoning, kimi-k2.5]

channels:
  whatsapp:
    enabled: true
    allowlist: []
    blocklist: []
  irc:
    enabled: false
    host: "irc.libera.chat"
    port: 6697
    tls: true
    nick: "MiriBot"
    user: "miri"
    realname: "Miri Autonomous Agent"
    channels: ["#miri-test"]
    allowlist: []
    blocklist: []
    nickserv:
      enabled: false
      password: ""
